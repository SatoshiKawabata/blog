---
templateKey: blog-post
title: Gatsby製のサイトにrobots.txtを配置して検索サイトのクロールを制御する
date: 2019-09-26T14:28:27.325Z
description: >-
  検索サイトにインデックスさせたくないページがあったり、まだ公開したくないページなどがある場合、robots.txtで検索サイトのクローラを制御することができます。 
  Gatsby製のサイトでrobots.txtを配置したかったのでプラグインを試してみました。
---
検索サイトにインデックスさせたくないページがあったり、まだ公開したくないページなどがある場合、robots.txtで検索サイトのクローラを制御することができます。

Gatsby製のサイトでrobots.txtを配置したかったのでプラグインを試してみました。

## gatsby-plugin-robots-txt プラグインを使う

[gatsby-plugin-robots-txt](https://www.gatsbyjs.org/packages/gatsby-plugin-robots-txt/)

`gatsby-config.js`に以下の設定項目を追加します。

```
module.exports = {
  plugins: [
    {
      resolve: 'gatsby-plugin-robots-txt',
      options: {
        host: 'https://www.example.com',
        sitemap: 'https://www.example.com/sitemap.xml',
        policy: [{ userAgent: '*', allow: '/' }]
      }
    }
  ]
};
```

上記の設定で下記の`robots.txt`が作成されます。

```
User-agent: *
Allow: /
Sitemap: https://www.example.com/sitemap.xml
Host: https://www.example.com

```

## デプロイする環境ごとに設定を変える

例えばS3にstaging環境を用意している場合などはクロールさせたくありません。そういう場合は環境変数で場合分けすることができます。

`gatsby-config.js`に`env`を追加して`development`と`production`を追加します。なぜか`staging`だとちゃんと動作しませんでした。

```
module.exports = {
  plugins: [
    {
      resolve: 'gatsby-plugin-robots-txt',
      options: {
        host: 'https://www.example.com',
        sitemap: 'https://www.example.com/sitemap.xml',
        env: {
          development: {
            policy: [{ userAgent: '*', disallow: '/' }]
          },
          production: {
            policy: [{ userAgent: '*', allow: '/' }]
          }
        }
      }
    }
  ]
};
```

コマンドに`NODE_ENV`を追加します。

```
"build:development": "NODE_ENV=development gatsby build",
"build:production": "NODE_ENV=production gatsby build,
```

これでdevelopment環境とproduction環境のrobots.txtを出し分けすることができます。

## robots.txtの設定項目

まぁググればすぐにわかるんですが、よく使う項目をまとめておきます。

### `host`

`robots.txt`が置かれているサイトのホストです。

### `sitemap`

sitemap.xmlの場所を記述してクロールを促します。

### `policy`

クロールを制御する細かい設定を書くことができます。[このページ](https://github.com/itgalaxy/generate-robotstxt#usage)を見ればなんとなく書き方がつかめるかと思います。

だいたい使うのは以下の3つじゃないでしょうか。

#### `userAgent`

検索サイトのクローラです。`Googlebot`などを書きます。全てのクローラに対して設定したい場合は`*`を書きます。

#### `allow`

クロールするページを書きます。全てのページをクロールする場合は`/`と書きます。

#### `disallow`

クロールさせないページを書きます。全てのページをクロールする場合は`/`と書きます。
